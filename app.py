import streamlit as st
import pandas as pd
import jieba
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import time

# --- 1. é¡µé¢é…ç½® (å¿…é¡»æ˜¯ç¬¬ä¸€è¡Œ) ---
st.set_page_config(
    page_title="å—é‚®æ–°é—»æœ",
    page_icon="ğŸ“",
    layout="centered"
)

# --- 2. æ ¸å¿ƒé€»è¾‘ (å¸¦ç¼“å­˜ä¼˜åŒ–) ---
# @st.cache_data æ˜¯ Streamlit çš„ç¥å™¨
# å®ƒçš„ä½œç”¨æ˜¯ï¼šåªæœ‰ç¬¬ä¸€æ¬¡è¿è¡Œä¼šåŠ è½½æ•°æ®å’Œè®­ç»ƒæ¨¡å‹ï¼Œåç»­åˆ·æ–°é¡µé¢ç›´æ¥ç”¨ç¼“å­˜
# å¦åˆ™ç”¨æˆ·æ¯æœä¸€æ¬¡éƒ½è¦é‡æ–°è®­ç»ƒæ¨¡å‹ï¼Œé€Ÿåº¦ä¼šå¾ˆæ…¢
@st.cache_data
def load_data_and_model():
    # A. è¯»å–æ•°æ®
    try:
        df = pd.read_csv("njupt_news_cut.csv", keep_default_na=False)
    except FileNotFoundError:
        return None, None, None
    
    corpus = df['cut_content'].values
    
    # B. è®­ç»ƒæ¨¡å‹
    vectorizer = TfidfVectorizer(max_features=10000)
    tfidf_matrix = vectorizer.fit_transform(corpus)
    
    return df, vectorizer, tfidf_matrix

# åŠ è½½èµ„æº
df, vectorizer, tfidf_matrix = load_data_and_model()

# --- 3. ä¾§è¾¹æ  (é¡¹ç›®ä»‹ç» - ä½ çš„ç®€å†äº®ç‚¹) ---
with st.sidebar:
    st.image("https://upload.wikimedia.org/wikipedia/zh/4/44/Logo_of_NJUPT.svg", width=200)
    st.markdown("## å…³äºæœ¬é¡¹ç›®")
    st.write("è¿™æ˜¯ä¸€ä¸ªåŸºäº **TF-IDF** ç®—æ³•çš„å‚ç›´æœç´¢å¼•æ“ï¼Œä¸“ä¸ºæ£€ç´¢å—é‚®æ ¡å†…æ–°é—»è®¾è®¡ã€‚")
    
    st.markdown("### ğŸ› ï¸ æŠ€æœ¯æ ˆ")
    st.markdown("- **çˆ¬è™«**: Requests + BeautifulSoup")
    st.markdown("- **æ•°æ®æ¸…æ´—**: Pandas + Jieba")
    st.markdown("- **æ ¸å¿ƒç®—æ³•**: Scikit-learn (TF-IDF + Cosine Similarity)")
    st.markdown("- **ç•Œé¢**: Streamlit")
    
    st.markdown("---")
    st.write("Designed by å—é‚®å¤§ä¸‰å­¦ç”Ÿ")

    

# --- 4. ä¸»ç•Œé¢ (UI) ---
st.title("ğŸ“ å—é‚®æ ¡å†…æ–°é—»æœç´¢å¼•æ“")
st.markdown("è¾“å…¥å…³é”®è¯ï¼Œç¬é—´æ‰¾å›ä¸¢å¤±çš„æ ¡å›­è®°å¿†...")

# æ£€æŸ¥æ•°æ®æ˜¯å¦åŠ è½½æˆåŠŸ
if df is None:
    st.error("âŒ é”™è¯¯ï¼šæ‰¾ä¸åˆ° njupt_news_cut.csvï¼è¯·å…ˆè¿è¡Œ Level 2 çš„æ¸…æ´—è„šæœ¬ã€‚")
    st.stop()

# æœç´¢æ¡†
query = st.text_input("è¯·è¾“å…¥å…³é”®è¯ (å¦‚ï¼šå¥–å­¦é‡‘ã€è€ƒç ”ã€é£Ÿå ‚)", placeholder="Try searching 'è®¡ç®—æœº'...")
search_btn = st.button("ğŸ” ç«‹å³æœç´¢")

# --- 5. æœç´¢å“åº”é€»è¾‘ ---
if search_btn and query:
    start_ts = time.time()
    
    # A. å¤„ç†æŸ¥è¯¢è¯
    query_cut = " ".join(jieba.lcut(query))
    
    # B. å‘é‡åŒ– & è®¡ç®—ç›¸ä¼¼åº¦
    query_vec = vectorizer.transform([query_cut])
    sim_scores = cosine_similarity(query_vec, tfidf_matrix).flatten()
    
    # C. æ’åºå–å‰ 10
    sorted_indices = sim_scores.argsort()[::-1][:10]
    
    # D. å±•ç¤ºç»“æœ
    st.markdown("### ğŸ“Š æœç´¢ç»“æœ")
    
    found_count = 0
    for idx in sorted_indices:
        score = sim_scores[idx]
        if score < 0.05: continue # è¿‡æ»¤ä½ç›¸å…³æ€§
        
        found_count += 1
        row = df.iloc[idx]
        
        # ä½¿ç”¨ Streamlit çš„ container ç¾åŒ–å±•ç¤º
        with st.container():
            # æ ‡é¢˜å¸¦é“¾æ¥
            st.markdown(f"#### [{row['title']}]({row['link']})")
            
            # æ˜¾ç¤ºåŒ¹é…åº¦è¿›åº¦æ¡
            st.progress(float(score), text=f"ç›¸å…³åº¦: {score:.2f}")
            
            # æ‘˜è¦
            content_preview = str(row['content'])[:80] + "..."
            st.caption(content_preview)
            
            st.divider() # åˆ†å‰²çº¿
            
    if found_count == 0:
        st.warning(f"æ²¡æœ‰æ‰¾åˆ°å…³äº '{query}' çš„æ–°é—»ï¼Œæ¢ä¸ªè¯è¯•è¯•ï¼Ÿ")
    else:
        cost = time.time() - start_ts
        st.success(f"å…±æ‰¾åˆ° {found_count} æ¡ç›¸å…³ç»“æœï¼Œè€—æ—¶ {cost:.4f} ç§’")
